{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this notebook I going to work with sample data from dog breed inentification competiotion (images only of 2 breeds present in sample data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "#current_dir = os.getcwd()\n",
    "DATA_HOME_DIR =  '/home/denys/projects/ml-models/kaggle/dog-breed-identification/data/'\n",
    "#DATA_HOME_DIR =  '/home/denys/projects/ml-models/kaggle/dog-breed-identification/data/sample/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "import utils; reload(utils)\n",
    "from utils import plot_learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(\"data/labels.csv\")\n",
    "\n",
    "def get_breed(id):\n",
    "    return labels_df.loc[labels_df[\"id\"] == id][\"breed\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size=(244,244)\n",
    "\n",
    "def get_batches(dirname, gen=image.ImageDataGenerator(), shuffle=True, batch_size=4, class_mode='categorical',\n",
    "                target_size=target_size):\n",
    "    return gen.flow_from_directory(dirname, target_size=target_size, \n",
    "                                   class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "def get_data(path):\n",
    "    batches = get_batches(path, shuffle=False, batch_size=1, class_mode=None)\n",
    "    return np.concatenate([batches.next() for i in range(batches.samples)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9225 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = get_data(DATA_HOME_DIR+'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(DATA_HOME_DIR+'train_244_244.data', train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = load_array(DATA_HOME_DIR+'train_244_244.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9225 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(DATA_HOME_DIR + 'train', shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = batches.classes\n",
    "train_labels = onehot(train_classes)\n",
    "train_filenames = batches.filenames\n",
    "train_class_indices = batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = DATA_HOME_DIR + \"models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(224,224, 3)),\n",
    "            Conv2D(32,(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Conv2D(64,(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Conv2D(128,(3,3), activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Flatten(),\n",
    "            BatchNormalization(),\n",
    "            Dense(2, activation='softmax')\n",
    "        ])\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, batches, val_batches, initialisation=False):\n",
    "    if initialisation:\n",
    "        model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.fit_generator(batches, steps_per_epoch=batches.n / batch_size, epochs=10, validation_data=val_batches, \n",
    "                    validation_steps=val_batches.n, verbose=1, callbacks=[csv_logger])\n",
    "\n",
    "    model.optimizer.lr = 0.00001\n",
    "    model.fit_generator(batches, steps_per_epoch=batches.n / batch_size, epochs=100, validation_data=val_batches, \n",
    "                        validation_steps=val_batches.n, verbose=1, callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=3, random_state=1, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "csv_logger = CSVLogger('vgg16_0128_2020.log', append=True)\n",
    "\n",
    "batch_size=10\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "kf.get_n_splits(train_data)\n",
    "print(kf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=3, random_state=1, shuffle=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() takes exactly 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-fa5fd512b48c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() takes exactly 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "kf.get_n_splits(train_data)\n",
    "print(kf) \n",
    "\n",
    "batch_size=10\n",
    "\n",
    "base_model = model()\n",
    "\n",
    "init = False\n",
    "for train_index, valid_index in kf.split(train_data):\n",
    "    print train_index, valid_index\n",
    "    train_split, valid_split = train_data[train_index], train_data[valid_index]\n",
    "    train_labels_split, valid_labels_split = train_labels[train_index], train_labels[valid_index]\n",
    "\n",
    "    batches = image.ImageDataGenerator(rotation_range=15, width_shift_range=0.1, \n",
    "                               height_shift_range=0.1, zoom_range=0.2, horizontal_flip=True).flow(train_split, train_labels_split, batch_size=batch_size, shuffle=True)\n",
    "    val_batches = image.ImageDataGenerator().flow(valid_split, valid_labels_split, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    train(base_model, batches, val_batches, init)\n",
    "    init = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model.save_weights(model_path[:-len(\"models/sample/\")] + \"models/new_model_sample.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_data, valid_labels, valid_filenames, valid_class_indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doberman': 0, 'pomeranian': 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21754244, 0.9333334]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.test_on_batch(valid_data, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that simple model was not able capture all usefull properties of data, \n",
    "so I tried to create more complicated model with 3 convolution layers and was right my test accuracy is 0.93 now\n",
    "\n",
    "Now lets try transfered learning with next models:\n",
    "    >VGG16\n",
    "    >ResNet50\n",
    "    >InceptionV3\n",
    "    >InceptionResNetV2\n",
    "    >MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "\n",
    "# = {\"vgg16\": VGG16(weights='imagenet', include_top=False),\n",
    "         # \"resnet50\" : ResNet50(weights='imagenet', include_top=False),\n",
    "          #\"InceptionV3\": InceptionV3(weights='imagenet', include_top=False),\n",
    "          #\"InceptionResNetV2\": InceptionResNetV2(weights='imagenet', include_top=False)}\n",
    " #        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg16\n",
      "Epoch 1/30\n",
      "615/615 [==============================] - 449s 730ms/step - loss: 6.6826 - acc: 0.5580 - val_loss: 6.7118 - val_acc: 0.5516\n",
      "Epoch 2/30\n",
      "615/615 [==============================] - 446s 726ms/step - loss: 6.5704 - acc: 0.5750 - val_loss: 6.6197 - val_acc: 0.5616\n",
      "Epoch 3/30\n",
      "615/615 [==============================] - 446s 726ms/step - loss: 6.5822 - acc: 0.5750 - val_loss: 6.7159 - val_acc: 0.5621\n",
      "Epoch 4/30\n",
      "615/615 [==============================] - 446s 725ms/step - loss: 6.5096 - acc: 0.5834 - val_loss: 6.6363 - val_acc: 0.5700\n",
      "Epoch 5/30\n",
      "615/615 [==============================] - 446s 726ms/step - loss: 6.5264 - acc: 0.5807 - val_loss: 6.7379 - val_acc: 0.5511\n",
      "Epoch 6/30\n",
      "615/615 [==============================] - 446s 725ms/step - loss: 6.4790 - acc: 0.5873 - val_loss: 6.6878 - val_acc: 0.5517\n",
      "Epoch 7/30\n",
      "614/615 [============================>.] - ETA: 0s - loss: 6.4808 - acc: 0.5853"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "for model_name, base_model in {\"vgg16\": VGG16(weights='imagenet', include_top=False)}.items():\n",
    "    print model_name\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    predictions = Dense(120, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer='adam', metrics=[\"accuracy\"], loss='categorical_crossentropy')\n",
    "    model.optimiser = lr=0.00005\n",
    "    \n",
    "    model.load_weights(model_path + \"/\" + model_name + '_0128_1556.h5')\n",
    "    \n",
    "    for train_index, valid_index in kf.split(train_data):\n",
    "        train_split, valid_split = train_data[train_index], train_data[valid_index]\n",
    "        train_labels_split, valid_labels_split = train_labels[train_index], train_labels[valid_index]\n",
    "\n",
    "        #rotation_range=15, width_shift_range=0.1, \n",
    "                               #height_shift_range=0.1, zoom_range=0.2, horizontal_flip=True\n",
    "        batches = image.ImageDataGenerator().flow(train_split, train_labels_split, batch_size=batch_size, shuffle=True)\n",
    "        val_batches = image.ImageDataGenerator().flow(valid_split, valid_labels_split, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    \n",
    "        model.fit_generator(batches, steps_per_epoch=batches.n / batch_size, epochs=30, validation_data=val_batches, \n",
    "                        validation_steps=val_batches.n, verbose=1, callbacks=[csv_logger])\n",
    "            \n",
    "    model.save_weights(model_path + \"/\" + model_name + '_0128_2020.h5')\n",
    "   # print model.test_on_batch(valid_data, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\"vgg16_0128_1557.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InceptionResNetV2\n",
      "Epoch 1/10\n",
      "615/615 [==============================] - 536s 871ms/step - loss: 2.0528 - acc: 0.5356 - val_loss: 1.3072 - val_acc: 0.7303\n",
      "Epoch 2/10\n",
      "615/615 [==============================] - 522s 849ms/step - loss: 1.3119 - acc: 0.6979 - val_loss: 1.5568 - val_acc: 0.7401\n",
      "Epoch 7/10\n",
      "615/615 [==============================] - 522s 849ms/step - loss: 1.2715 - acc: 0.7205 - val_loss: 1.5353 - val_acc: 0.7501\n",
      "Epoch 8/10\n",
      "615/615 [==============================] - 522s 850ms/step - loss: 1.2840 - acc: 0.7189 - val_loss: 1.6225 - val_acc: 0.7466\n",
      "Epoch 9/10\n",
      "615/615 [==============================] - 523s 850ms/step - loss: 1.3214 - acc: 0.7218 - val_loss: 1.6152 - val_acc: 0.7473\n",
      "Epoch 10/10\n",
      "615/615 [==============================] - 523s 850ms/step - loss: 1.3093 - acc: 0.7249 - val_loss: 1.6137 - val_acc: 0.7495\n",
      "Epoch 1/10\n",
      "615/615 [==============================] - 549s 893ms/step - loss: 1.5033 - acc: 0.7031 - val_loss: 1.4089 - val_acc: 0.8041\n",
      "Epoch 2/10\n",
      "615/615 [==============================] - 524s 851ms/step - loss: 1.5234 - acc: 0.7128 - val_loss: 1.3808 - val_acc: 0.8042\n",
      "Epoch 3/10\n",
      "615/615 [==============================] - 524s 851ms/step - loss: 1.5071 - acc: 0.7135 - val_loss: 1.4965 - val_acc: 0.7969\n",
      "Epoch 4/10\n",
      "615/615 [==============================] - 523s 851ms/step - loss: 1.5519 - acc: 0.7091 - val_loss: 1.4451 - val_acc: 0.7987\n",
      "Epoch 5/10\n",
      "615/615 [==============================] - 524s 852ms/step - loss: 1.4854 - acc: 0.7166 - val_loss: 1.5462 - val_acc: 0.7951\n",
      "Epoch 6/10\n",
      "614/615 [============================>.] - ETA: 0s - loss: 1.4982 - acc: 0.7243"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "for model_name, base_model in models.items():\n",
    "    print model_name\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    predictions = Dense(120, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer='rmsprop', metrics=[\"accuracy\"], loss='categorical_crossentropy')\n",
    "    \n",
    "    for train_index, valid_index in kf.split(train_data):\n",
    "        train_split, valid_split = train_data[train_index], train_data[valid_index]\n",
    "        train_labels_split, valid_labels_split = train_labels[train_index], train_labels[valid_index]\n",
    "\n",
    "        batches = image.ImageDataGenerator(rotation_range=15, width_shift_range=0.1, \n",
    "                               height_shift_range=0.1, zoom_range=0.2, horizontal_flip=True).flow(train_split, train_labels_split, batch_size=batch_size, shuffle=True)\n",
    "        val_batches = image.ImageDataGenerator().flow(valid_split, valid_labels_split, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    \n",
    "        model.fit_generator(batches, steps_per_epoch=batches.n / batch_size, epochs=10, validation_data=val_batches, \n",
    "                        validation_steps=val_batches.n, verbose=1, callbacks=[csv_logger])\n",
    "            \n",
    "    model.save_weights(model_path + \"/\" + model_name + '.h5')\n",
    "   # print model.test_on_batch(valid_data, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.076204494, 0.9666667]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test_on_batch(valid_data, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
